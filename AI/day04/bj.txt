机器学习
a^2+b^2=c^2
c = np.sqrt(a ** 2 + b ** 2)
理性主义
经验主义
通过大量的数据，让计算机系统掌握数据之间的内在联系，进而对未知的结果做出预测，这样的系统谓之机器学习系统。
机器学习的过程就是发现数据之间内在联系的过程。
第一步：数据采集、挖掘和清洗――买菜
第二步：数据预处理――洗菜和切菜
第三步：模型选择――制定菜谱
第四步：模型训练――烹饪
第五步：模型测试――试吃
第六步：使用模型――上桌
一、数据预处理
1.均值移除(标准化)
                         x2
                    +-----+  
输入数据  -> | 模型 | <->  输出数据
                    +-----+
                       训练
1                                     2
2                                     4
3                                     6
...
100                x2            200   == 200 OK
...
                      测试
1000              x2            2000 -> 信任，业务
                      使用
                                          
样本矩阵  特征1 特征2 特征3 ... 特征n -> 标签向量
样本1         x        x        x      ...     x                y
样本2         x        x        x      ...     x                y
样本3         x        x        x      ...     x                y
                                              ...
样本m        x        x        x      ...     x                y
一行一样本，一列一特征。
年龄  学历  学校  工作经历  -> 薪资
  25   专科  普通 没有               3000
  28   本科  985  2                   6000
  35   博士  211  5                   10000
  ...
  27   硕士  普通  3                   ?
使样本矩阵中的各列的平均值为0，标准差为1，即将每个特征的基准位置和分散范围加以统一，在数量级上尽可能接近，对模型的预测结果做出均等的贡献。
... 特征i ...
       a
       b
       c
m=(a+b+c)/3
a' = a - m
b' = b - m
c' = c - m
m'=(a'+b'+c')/3
     =(a-m+b-m+c-m)/3
     =(a+b+c)/3 - m
     = 0
s' = sqrt((a'^2+b'^2+c'^2)/3)
(a'^2+b'^2+c'^2)/3 = s'^2
(a'^2+b'^2+c'^2)=3s'^2
a" = a'/s'
b" = b'/s'
c" = c'/s'
s" = sqrt((a"^2+b"^2+c"^2)/3)
    = sqrt((a'^2/s'^2+b'^2/s'^2+c'^2/s'^2)/3)
    = sqrt((a'^2+b'^2+c'^2)/(3s'^2))
    = 1
代码：std.py
pip install scikit-learn
import sklearn.preprocessing as sp
sp.scale(原始样本矩阵)->均值移除样本矩阵
2.范围缩放
         语文   数学          英语
张三    90    10  100    5  100
李四    80      8    80    2     40
王五  100      5    50    1     20
将样本矩阵中的每一列通过线性变换，是各列的最小值和最大值为某个给定值，即分布在相同的范围中。
线性变换: kx + b = y
                kcol_min + b = min
                kcol_max + b = max
                /col_min 1\  x /k\ = /min\
                \col_max 1/    \b/     \max/
                --------------   ----     -------
                          a              x            b
                                          = numpy.linalg.solve(a, b)
代码：mms.py
mms = sp.MinMaxScaler(feature_range=(min, max))
mms.fit_transform(原始样本矩阵)->范围缩放样本矩阵
3.归一化
          C/C++ Java Python PHP
2016 30         40    10         5      /85
2017 30         35    40         1      /106
2018 20         30    50         0      /100
...
                           hello world apple tarena ...
xxxxxxxxxxxxx     0        5          1        2     /8
xxxxxxxxxxxxx     1        0          0        3     /4
xxxxxxxxxxxxx     2        0          2        0     /4
...
代码：nor.py
sp.normalize(原始样本矩阵, norm='l1')->归一化样本矩阵
L1范数：向量中各元素绝对值之和。
L2范数：向量中各元素的平方之和。
4.二值化
根据业务的需求，设定一个阈值，样本矩阵中大于阈值的元素被置换为1，小于或等于阈值的元素被置换为0，整个样本矩阵被处理为只由0和1组成样本空间。
代码：bin.py
bin = sp.Binarizer(threshold=阈值)
bin.transform(原始样本矩阵)->二值化样本矩阵
5.独热编码
1         3          2
7         5          4
1         8          6
7         3          9
1: 10  3: 100  2: 1000
7: 01  5: 010  4: 0100
           8: 001 6: 0010
                       9: 0001
1 0 1 0 0 1 0 0 0
0 1 0 1 0 0 1 0 0
1 0 0 0 1 0 0 1 0
0 1 1 0 0 0 0 0 1
代码：ohe.py
ohe = sp.OneHotEncoder(
    sparse=是否压缩, dtype=元素类型)
ohe.fit_transform(原始样本矩阵)->独热编码样本矩阵
6.标签编码
年龄  学历  学校  工作经历  -> 薪资
  25   专科  普通 没有               low
  28   本科  985  2                   med
  35   博士  211  5                   high
  ...
  27   硕士  普通  3                   ?
将字符串形式的特征值编码成数字，便于数学运算。
low med high
排序:
high low med
0       1     2
编码：
1 2 0
代码：lab.py
lbe = sp.LabelEncoder()
lbe.fit_transform(原始样本列) -> 标签编码列，构建字典
lbe.transform(原始样本列) -> 标签编码列，使用字典
lbe.inverse_transform(标签编码列)->原始样本列，使用字典
二、机器学习基本类型
1.有监督学习：用已知的输入和输出训练学习模型，直到模型给出的预测输出与已知实际输出间的误差小到可以接受的程度为止。
x1  -> y1
x2  -> y2
x3  -> y3
...
y = f(x)
x1 -> y1'
x2 -> y2'
x3 -> y3'
...
1)回归问题：输出数据是无限可能的连续值。
2)分类问题：输出数据是有限的几个离散值。
2.无监督学习：在输出数据未知的前提下，利用模型本身发现输入数据的内部特征，将其划分为不同的族群。
聚类问题
3.半监督学习：利用相对较小的已知集训练模型，使其获得基本的预测能力，当模型遇到未知输出的新数据时，可以根据其与已知集的相似性，预测其输出。
三、线性回归
  x   ->   y
0.5       5.0
0.6       5.5
0.8       6.0
1.1       6.8
1.4       7.0
y = w0 + w1x
            SIGMA((y - (w0 + w1x))^2)
loss = --------------------------------
                                2
目标：寻找最理想的w0和w1，使loss尽可能的小。
dloss
------ = -SIGMA(y - (w0 + w1x))
dw0
dloss
------ = -SIGMA((y - (w0 + w1x))x)
w1
                      dloss
w0 = w0 - n ------
                      dw0
                      dloss
w1 = w1 - n -------
                      dw1
代码：bgd.py
import sklearn.linear_model as lm
model = lm.LinearRegression()       # 创建模型
model.fit(训练输入, 训练输出)            # 训练模型
预测输出 = model.predict(预测输入) # 预测输出
代码：line.py
import pickle
with open(模型文件路径) as f:
    pickle.dump(model, f) # 将学习模型保存到文件
代码：dump.py
with open(模型文件路径) as f:
    model = pickle.load(f) # 从文件中载入学习模型
代码：load.py
四、岭回归
loss = J(w0, w1) +
           正则强度(惩罚力度) x 正则项(x, y, w0, w1)
model = lm.Ridge(正则强度, fit_intercept=True,
                                max_iter=最大迭代次数)
fit/predict ...
代码：rdg.py
五、多项式回归
y = w0 + w1x + w2x^2 + w3x^3 + ... + wnx^n
                     x1          x2             x3                    xn
y = w0 + w1x1 + w2x2 + w3x3 + ... + wnxn
多项式特征扩展：增加高次项作为扩展特征值；
沿用线性回归对增补了扩展特征值后的样本矩阵进行回归。
import sklearn.pipeline as pl
import sklearn.preprocessing as sp
...
sp.PolynomialFeatures(n) -> 多项式特征扩展器
lm.LinearRegression() -> 线性回归器
pl.make_pipeline(多项式特征扩展器, 线性回归器) -> 管线
管线.fit()
管线.predict() -> 预测输出
x -> 多项式特征扩展器 -> x1, x2, ..., xn -> 线性回归器 ->
                                         x^1 x^2   x^n       w0 w1 ... wn
代码：poly.py
欠拟合：模型中参数并不能以最佳损失值的形式反映输入和输出之间的关系。因此，无论是用训练集输入还是测试集输入，由模型给出的预测输出都不能以较小的误差接近实际的输出。
过拟合：模型中的参数过分依赖或者倾向于训练数据，反而缺乏一般性，即导致泛化程度的缺失。因此，当使用训练集输入时，模型通常可以给出较高精度的预测输出，而使用测试集输入，模型的表现却非常差。
六、决策树回归和分类
相似的输入必会产生相似的输出。
年龄：0 - 青年(20-40)，1 - 中年(40-60)，2 - 老年(60-80)
性别：0 - 女性，1 - 男性
学历：0 - 大专，1 - 大本，2 - 硕士，3 - 博士
工作年限：0 - <3，1 - [3,5]，2 - >5
月薪：0 - 低，1 - 中，2 - 高
年龄    性别    学历    工作年限 -> 月薪
  0         1         1             0           5000    0
  0         1         0             1           6000    1
  1         0         2             2           8000    1
  2         1         3             2           50000  2
  ...
  1         1         2             1               ?   

1 1 2 1 -> 6000  1 \
1 1 2 1 -> 7000  1  | mean -> 预测值
1 1 2 1 -> 5000  0  | 投票    -> 预测值
...                             /
                                     根表 

      年龄表0               年龄表1              年龄表2

性别表0 性别表1 性别表0 性别表1 性别表0 性别表1

 0      1     2    3  0 1 2 3  0 1 2 3  0 1 2 3  0 1 2 3  0 1 2 3  

012 012 012 012

完全决策树：使用所有的特征作为子表划分的依据，树状结构复杂，构建和预测速度慢。
非完全决策树：根据信息熵减少量最大的原则，优先选择部分特征划分子表，在输入相似的条件下，预测相似的输出。
集合算法：通过不同的方式构建出多棵决策树模型，分别作出预测，将它们给出的预测值通过平均或投票的方式综合考虑，得出最终的预测结果。
A.自助聚合：从总样本空间中，以有放回抽样的方式随机挑选部分样本构建决策树，共构造B棵树，由这B棵树分别对未知样本进行预测，给出B个预测结果，经由平均或投票产生最后的输出。
B.随机森林：在自助聚合算法的基础上，每次抽样不但随机选择样本，而且也随机选择特征，来构建B棵决策树，以此泛化不同特征对预测结果的影响。
C.正向激励：开始为每个样本分配初始权重，构建决策树，对训练集中的样本进行预测，针对预测错误的样本，增加其权重，再构建决策树，重复以上过程，共得到B棵决策树，由这B棵树分别对未知样本进行预测，给出B个预测结果，经由平均或投票产生最后的输出。
代码：house.py
x x x x x   y
x x x x x   y   
x x x x x   y
x x x x x   y  80%
-------------------
x x x x x   y 20%
x x x x x   y
特征重要性：决策树模型在构建树状结构时，优先选择对输出结果影响最大，即可产生最大信息熵减少量的特征进行子表划分，因此该模型可以按照特征的重要程度进行排序，即特征重要性序列。不同的模型因算法不同，所得到的特征重要性序列也会有所不同。另外，训练数据的细化程度也会影响模型对特征重要性的判断。
代码：fi.py、bike.py
七、简单分类
x1 x2 -> y
3   1        0
2   5        1
1   8        1
6   4        0
5   2        0
3   5        1
4   7        1
4  -1        0
模型：
if x1 < x2 then y = 1
if x1 > x2 then y = 0
2   9        ? -> 1
7   3        ? -> 0
代码：simple.py
八、逻辑回归
x1 x2 -> y
y = w0 + w1x1 + w2x2
                             1
y = ---------------------------------
       1 + e^-(w0 + w1x1 + w2x2)
sigmoid函数
0.1\
0.2 |
0.3 | 0
0.4 |
0.5 /
0.6 \
0.7 | 1
0.8 |
0.9 /
model = lm.LogisticRegression(solver='liblinear',
                                                       C=正则强度)
fit/predict
代码：log.py
x x ... x -> A 1 0 0
x x ... x -> A 1 0 0
x x ... x -> B 0 1 0
x x ... x -> C 0 0 1
x x ... x -> B 0 1 0
x x ... x -> C 0 0 1
...
x x ... x -> ? 0.7 0.4 0.9
                     A    B    C -> C
代码：mlog.py
九、朴素贝叶斯分类
P(A) - A事件发生的概率
P(A,B) - A和B两个事件同时发生的概率，联合概率
P(A|B) - 在B事件发生的条件下A事件发生的概率，条件概率
贝叶斯定理：P(A,B) = P(B)P(A|B)
                     P(B,A) = P(A)P(B|A)
P(B)P(A|B) = P(A)P(B|A)
                P(A)P(B|A)
P(A|B) = -------------
                      P(B)
x1 x2 ... xn -> ?
\_________/
        X         -> 0 0.2
                         1 0.3   <-
                         2 0.05 
X样本属于C类别的概率是多少？
                P(C)P(X|C)
P(C|X) = ------------    (贝叶斯定理)
                     P(X)
P(C)P(X|C)
= P(C,X)
= P(X,C)
= P(x1,x2,x3,C)
= P(x1|x2,x3,C)P(x2,x3,C)
= P(x1|x2,x3,C)P(x2|x3,C)P(x3,C)
= P(x1|x2,x3,C)P(x2|x3,C)P(x3|C)P(C)
朴素：条件独立，所有的特征值彼此没有任何依赖性。
= P(x1|C)P(x2|C)P(x3|C)P(C)
1 7 9 -> 0
...
8 2 0 -> 0
...
6 4 3 -> 0
...
1 9 2 -> 0
...
1 2 3 -> ? 0 
拥有足够的训练样本，可以通过统计的方法获得各个特征值的概率，或者在已知每个特征所服从概率分布的前提下，利用概率密度或者概率质量函数计算出每个特征值出现的概率。
import sklearn.naive_bayes as nb
model = nb.GaussianNB() # 正态分布朴素贝叶斯模型
fit/predict
代码：nb.py
import sklearn.model_selection as ms
ms.train_test_split(输入集合, 输出集合,
    test_size=测试集占比, random_state=随机种子)->
    训练输入, 测试输入, 训练输出, 测试输出
代码：split.py
十、分类器的性能指标
查准率：正确性，正确预测样本数/总预测样本数，[0, 1]
              对/(对+错)
召回率：完整性，正确预测样本数/总实际样本数，[0, 1]
              对/(对+漏)
F1得分：2 x 查准率 x 召回率 / (查准率 + 召回率), [0, 1]
交叉验证得分：
ms.cross_val_score(分类器, 输入集合, 输出集合,
   cv=验证次数, scoring=指标名称) -> 得分数组
指标名称：precision_weighted，查准率
                 recall_weighted, 召回率
                 f1_weighted, F1得分
代码：cv.py
混淆矩阵：
                  预测类别1  预测类别2  预测类别3
实际类别1          7               2               1
实际类别2          0              20              0
实际类别3          0               0              30
sm.confusion_matrix(实际输出, 预测输出)->混淆矩阵
代码：cm.py
分类报告：
根据混淆矩阵计算每个类别的查准率、召回率和F1得分。
查准率 = 对角线上的元素/其所在列元素之和
召回率 = 对角线上的元素/其所在行元素之和
sm.classification_report(实际输出, 预测输出)->分类报告
代码：cr.py
十一、基于决策树(随机森林)模型的分类
model = se.RandomForestClassifier(
    max_depth=最大树高, n_estimators=决策树数量,
    random_state=随机种子)
fit/predict
代码：car.py
训练曲线：模型中的某个超参数对模型性能的影响。
ms.validation_curve(模型, 输入集合, 输出集合,
    超参数名称, 超参数值表, cv=验证次数)->
                 验证1次  验证2次  验证3次 ...
超参数值1  F1得分  ...
超参数值2  ...
超参数值3
...
代码：vc.py
学习曲线：训练模型时，训练集的规模对模型性能的影响。
ms.learning_curve(模型, 输入集合, 输出集合,
    train_sizes=训练集大小列表, cv=验证次数)->
                      验证1次  验证2次  验证3次 ...
训练集大小1  F1得分  ...
训练集大小2  ...
训练集大小3
...
代码：lc.py
十二、支持向量机
1.线性，距离支持向量等间距且最大。
x1 x2 -> y
x1 x2 x3=f(x1, x2)            -> y
               = x1^2 + x2^2
2.对于线性不可分的样本，通过核函数升高维度，在高维度样本空间中，线性分割。
线性核函数：linear
多项式核函数：poly
径向基核函数：rbf
3.更好的分类边界：公平、准确
   支持非线性分类边界
基于线性核函数的SVM分类
代码：line.py
基于多项式核函数的SVM分类
代码：poly.py
基于径向基核函数的SVM分类
代码：rbf.py
均衡样本权重
svm.SVC(..., class_weight='balanced', ...)
代码：bal.py
置信概率：每个预测样本被预测为每个类别的概率。
model.predict_proba(预测样本输入)->
                  类别1 类别2 ...
样本输入1  0.8     0.2
样本输入2  0.3     0.7
...
代码：prob.py
越接近分类边界的样本，对应各个类别的置信概率差别就越小，相反越远离分类边界的样本，对应各个类别的置信概率差别就越大。
通过正交搜索寻找最优超参数组合。
ms.GridSearchCV(模型, 超参数组合列表, cv=验证次数)->
    对应最优组合的模型
超参数组合列表：[{参数名: [取值列表], ...}, {...}, ...]
代码：bhp.py
楼宇监控事件预测模型
evt.py
x -> y
1      10 A
2      20 A
3      30 B
4      40 B
5      50 C
6      60 C
7      70 D
8      80 D
5.8        C -> (50*1+60*2) / 3
[10,30) - A
[30,50) - B
[50,70) - C
[70,90) - D
代码：trf.py
十三、聚类
1.欧式距离
一维空间：P(x1), Q(x2)
PQ = |x1-x2| = sqrt((x1-x2)^2)
二维空间：P(x1,y1), Q(x2,y2)
PQ = sqrt((x1-x2)^2+(y1-y2)^2)
三维空间：P(x1,y1,z1), Q(x2,y2,z2)
PQ = sqrt((x1-x2)^2+(y1-y2)^2+(z1-z2)^2)
N维空间：对应特征值之差的平方和的平方根。
张三：1.75/60/22
李四：1.80/70/25
2.K均值算法
给聚类中心分配点。计算所有的训练样本与每个聚类中心的距离，把每个训练样本分配到与之距离最近的中心所在的类别中。计算每个聚类的几何中心，将聚类中心移动几何中心处，重复以上过程，直到每个聚类中心与其聚类中所有样本的几何中心重合或接近重合为止。
聚类数必须事先已知。通过基于聚类指标的超参数寻优，找到最佳的聚类数。
聚类中心的初始位置会影响最后的聚类结果。初始聚类中心足够分散，kmeans++。
代码：km.py
基于聚类的图像量化
代码：quant.py
3.均值漂移算法
将待划分聚类的样本空间视作服从某种概率分布规律的随机数据，通过计算随机数据的直方图，寻找与之最为匹配的概率密度函数，该函数的峰值点就是聚类的中心。
适用于概率特征明显聚类样本。
无需事先给出聚类数，模型可以自动识别聚类的个数。
代码：shift.py
4.凝聚层次算法
首先将每个样本都视作单独的聚类，如果聚类数大于事先给定的期望值，那么就在这些聚类中寻找距离最近的两个聚类凝聚成更大的聚类，以使总聚类数减少。不断重复以上就近凝聚的过程，直到聚类数达到预期为止。
代码：agglo.py
凝聚层次算法可以选择连续性最好的方式完成聚类划分，以支持带有明显连续特征的聚类问题。
代码：spiral.py
5.评价聚类效果的指标
好的聚类：内密外疏，同一个聚类中的样本足够密集，不同聚类中的样本足够疏远。
轮廓系数：
a：一个样本与同聚类其它样本的平均距离。
b：一个样本与最近的另一个聚类中各样本的平均距离。
                                          b - a
该样本的轮廓系数：s = ------------
                                      max(a, b)
整个模型的轮廓系数 = 所有样本的轮廓系数的平均值
好聚类，内密外疏，b很大，a很小，s->1
差聚类，内疏外密，b很小，a很大，s->-1
|s|->0, a接近于b, 聚类重叠，不具备可划分性。
代码：score.py
6.DBSCAN(带噪声的基于密度的聚类)算法
随机选择一个样本，作为圆心，以事先给定半径做圆，被该圆圈中的样本即与中心样本属于同一个聚类，再以这些样本为圆心重复以上过程，不断扩大聚类的范围，直到再无新样本加入为止，至此完成一个聚类的划分。使用同样的方法，在其余未被聚类的样本中，选择新的圆心，得到其它聚类。
外周样本：被划分某个聚类中，但不能为该聚类引入新的样本
孤立样本：不属于任何一个聚类的样本
除以上样本之外都属于核心样本。






