机器学习
a^2+b^2=c^2
c = np.sqrt(a ** 2 + b ** 2)
理性主义
经验主义
通过大量的数据，让计算机系统掌握数据之间的内在联系，进而对未知的结果做出预测，这样的系统谓之机器学习系统。
机器学习的过程就是发现数据之间内在联系的过程。
第一步：数据采集、挖掘和清洗――买菜
第二步：数据预处理――洗菜和切菜
第三步：模型选择――制定菜谱
第四步：模型训练――烹饪
第五步：模型测试――试吃
第六步：使用模型――上桌
一、数据预处理
1.均值移除(标准化)
                         x2
                    +-----+  
输入数据  -> | 模型 | <->  输出数据
                    +-----+
                       训练
1                                     2
2                                     4
3                                     6
...
100                x2            200   == 200 OK
...
                      测试
1000              x2            2000 -> 信任，业务
                      使用
                                          
样本矩阵  特征1 特征2 特征3 ... 特征n -> 标签向量
样本1         x        x        x      ...     x                y
样本2         x        x        x      ...     x                y
样本3         x        x        x      ...     x                y
                                              ...
样本m        x        x        x      ...     x                y
一行一样本，一列一特征。
年龄  学历  学校  工作经历  -> 薪资
  25   专科  普通 没有               3000
  28   本科  985  2                   6000
  35   博士  211  5                   10000
  ...
  27   硕士  普通  3                   ?
使样本矩阵中的各列的平均值为0，标准差为1，即将每个特征的基准位置和分散范围加以统一，在数量级上尽可能接近，对模型的预测结果做出均等的贡献。
... 特征i ...
       a
       b
       c
m=(a+b+c)/3
a' = a - m
b' = b - m
c' = c - m
m'=(a'+b'+c')/3
     =(a-m+b-m+c-m)/3
     =(a+b+c)/3 - m
     = 0
s' = sqrt((a'^2+b'^2+c'^2)/3)
(a'^2+b'^2+c'^2)/3 = s'^2
(a'^2+b'^2+c'^2)=3s'^2
a" = a'/s'
b" = b'/s'
c" = c'/s'
s" = sqrt((a"^2+b"^2+c"^2)/3)
    = sqrt((a'^2/s'^2+b'^2/s'^2+c'^2/s'^2)/3)
    = sqrt((a'^2+b'^2+c'^2)/(3s'^2))
    = 1
代码：std.py
pip install scikit-learn
import sklearn.preprocessing as sp
sp.scale(原始样本矩阵)->均值移除样本矩阵
2.范围缩放
         语文   数学          英语
张三    90    10  100    5  100
李四    80      8    80    2     40
王五  100      5    50    1     20
将样本矩阵中的每一列通过线性变换，是各列的最小值和最大值为某个给定值，即分布在相同的范围中。
线性变换: kx + b = y
                kcol_min + b = min
                kcol_max + b = max
                /col_min 1\  x /k\ = /min\
                \col_max 1/    \b/     \max/
                --------------   ----     -------
                          a              x            b
                                          = numpy.linalg.solve(a, b)
代码：mms.py
mms = sp.MinMaxScaler(feature_range=(min, max))
mms.fit_transform(原始样本矩阵)->范围缩放样本矩阵
3.归一化
          C/C++ Java Python PHP
2016 30         40    10         5      /85
2017 30         35    40         1      /106
2018 20         30    50         0      /100
...
                           hello world apple tarena ...
xxxxxxxxxxxxx     0        5          1        2     /8
xxxxxxxxxxxxx     1        0          0        3     /4
xxxxxxxxxxxxx     2        0          2        0     /4
...
代码：nor.py
sp.normalize(原始样本矩阵, norm='l1')->归一化样本矩阵
L1范数：向量中各元素绝对值之和。
L2范数：向量中各元素的平方之和。
4.二值化
根据业务的需求，设定一个阈值，样本矩阵中大于阈值的元素被置换为1，小于或等于阈值的元素被置换为0，整个样本矩阵被处理为只由0和1组成样本空间。
代码：bin.py
bin = sp.Binarizer(threshold=阈值)
bin.transform(原始样本矩阵)->二值化样本矩阵
5.独热编码
1         3          2
7         5          4
1         8          6
7         3          9
1: 10  3: 100  2: 1000
7: 01  5: 010  4: 0100
           8: 001 6: 0010
                       9: 0001
1 0 1 0 0 1 0 0 0
0 1 0 1 0 0 1 0 0
1 0 0 0 1 0 0 1 0
0 1 1 0 0 0 0 0 1
代码：ohe.py
ohe = sp.OneHotEncoder(
    sparse=是否压缩, dtype=元素类型)
ohe.fit_transform(原始样本矩阵)->独热编码样本矩阵
6.标签编码
年龄  学历  学校  工作经历  -> 薪资
  25   专科  普通 没有               low
  28   本科  985  2                   med
  35   博士  211  5                   high
  ...
  27   硕士  普通  3                   ?
将字符串形式的特征值编码成数字，便于数学运算。
low med high
排序:
high low med
0       1     2
编码：
1 2 0
代码：lab.py
lbe = sp.LabelEncoder()
lbe.fit_transform(原始样本列) -> 标签编码列，构建字典
lbe.transform(原始样本列) -> 标签编码列，使用字典
lbe.inverse_transform(标签编码列)->原始样本列，使用字典
二、机器学习基本类型
1.有监督学习：用已知的输入和输出训练学习模型，直到模型给出的预测输出与已知实际输出间的误差小到可以接受的程度为止。
x1  -> y1
x2  -> y2
x3  -> y3
...
y = f(x)
x1 -> y1'
x2 -> y2'
x3 -> y3'
...
1)回归问题：输出数据是无限可能的连续值。
2)分类问题：输出数据是有限的几个离散值。
2.无监督学习：在输出数据未知的前提下，利用模型本身发现输入数据的内部特征，将其划分为不同的族群。
聚类问题
3.半监督学习：利用相对较小的已知集训练模型，使其获得基本的预测能力，当模型遇到未知输出的新数据时，可以根据其与已知集的相似性，预测其输出。
三、线性回归
  x   ->   y
0.5       5.0
0.6       5.5
0.8       6.0
1.1       6.8
1.4       7.0
y = w0 + w1x
            SIGMA((y - (w0 + w1x))^2)
loss = --------------------------------
                                2
目标：寻找最理想的w0和w1，使loss尽可能的小。
dloss
------ = -SIGMA(y - (w0 + w1x))
dw0
dloss
------ = -SIGMA((y - (w0 + w1x))x)
w1
                      dloss
w0 = w0 - n ------
                      dw0
                      dloss
w1 = w1 - n -------
                      dw1
代码：bgd.py
import sklearn.linear_model as lm
model = lm.LinearRegression()       # 创建模型
model.fit(训练输入, 训练输出)            # 训练模型
预测输出 = model.predict(预测输入) # 预测输出
代码：line.py
import pickle
with open(模型文件路径) as f:
    pickle.dump(model, f) # 将学习模型保存到文件
代码：dump.py
with open(模型文件路径) as f:
    model = pickle.load(f) # 从文件中载入学习模型
代码：load.py
四、岭回归
loss = J(w0, w1) +
           正则强度(惩罚力度) x 正则项(x, y, w0, w1)
model = lm.Ridge(正则强度, fit_intercept=True,
                                max_iter=最大迭代次数)
fit/predict ...
代码：rdg.py
五、多项式回归
y = w0 + w1x + w2x^2 + w3x^3 + ... + wnx^n
                     x1          x2             x3                    xn
y = w0 + w1x1 + w2x2 + w3x3 + ... + wnxn
多项式特征扩展：增加高次项作为扩展特征值；
沿用线性回归对增补了扩展特征值后的样本矩阵进行回归。
import sklearn.pipeline as pl
import sklearn.preprocessing as sp
...
sp.PolynomialFeatures(n) -> 多项式特征扩展器
lm.LinearRegression() -> 线性回归器
pl.make_pipeline(多项式特征扩展器, 线性回归器) -> 管线
管线.fit()
管线.predict() -> 预测输出
x -> 多项式特征扩展器 -> x1, x2, ..., xn -> 线性回归器 ->
                                         x^1 x^2   x^n       w0 w1 ... wn
代码：poly.py
欠拟合：模型中参数并不能以最佳损失值的形式反映输入和输出之间的关系。因此，无论是用训练集输入还是测试集输入，由模型给出的预测输出都不能以较小的误差接近实际的输出。
过拟合：模型中的参数过分依赖或者倾向于训练数据，反而缺乏一般性，即导致泛化程度的缺失。因此，当使用训练集输入时，模型通常可以给出较高精度的预测输出，而使用测试集输入，模型的表现却非常差。
六、决策树回归和分类
相似的输入必会产生相似的输出。
年龄：0 - 青年(20-40)，1 - 中年(40-60)，2 - 老年(60-80)
性别：0 - 女性，1 - 男性
学历：0 - 大专，1 - 大本，2 - 硕士，3 - 博士
工作年限：0 - <3，1 - [3,5]，2 - >5
月薪：0 - 低，1 - 中，2 - 高
年龄    性别    学历    工作年限 -> 月薪
  0         1         1             0           5000    0
  0         1         0             1           6000    1
  1         0         2             2           8000    1
  2         1         3             2           50000  2
  ...
  1         1         2             1               ?   

1 1 2 1 -> 6000  1 \
1 1 2 1 -> 7000  1  | mean -> 预测值
1 1 2 1 -> 5000  0  | 投票    -> 预测值
...                             /
                                     根表 

      年龄表0               年龄表1              年龄表2

性别表0 性别表1 性别表0 性别表1 性别表0 性别表1

 0      1     2    3  0 1 2 3  0 1 2 3  0 1 2 3  0 1 2 3  0 1 2 3  

012 012 012 012

完全决策树：使用所有的特征作为子表划分的依据，树状结构复杂，构建和预测速度慢。
非完全决策树：根据信息熵减少量最大的原则，优先选择部分特征划分子表，在输入相似的条件下，预测相似的输出。
集合算法：通过不同的方式构建出多棵决策树模型，分别作出预测，将它们给出的预测值通过平均或投票的方式综合考虑，得出最终的预测结果。
A.自助聚合：从总样本空间中，以有放回抽样的方式随机挑选部分样本构建决策树，共构造B棵树，由这B棵树分别对未知样本进行预测，给出B个预测结果，经由平均或投票产生最后的输出。
B.随机森林：在自助聚合算法的基础上，每次抽样不但随机选择样本，而且也随机选择特征，来构建B棵决策树，以此泛化不同特征对预测结果的影响。
C.正向激励：开始为每个样本分配初始权重，构建决策树，对训练集中的样本进行预测，针对预测错误的样本，增加其权重，再构建决策树，重复以上过程，共得到B棵决策树，由这B棵树分别对未知样本进行预测，给出B个预测结果，经由平均或投票产生最后的输出。
代码：house.py
x x x x x   y
x x x x x   y   
x x x x x   y
x x x x x   y  80%
-------------------
x x x x x   y 20%
x x x x x   y
特征重要性：决策树模型在构建树状结构时，优先选择对输出结果影响最大，即可产生最大信息熵减少量的特征进行子表划分，因此该模型可以按照特征的重要程度进行排序，即特征重要性序列。不同的模型因算法不同，所得到的特征重要性序列也会有所不同。另外，训练数据的细化程度也会影响模型对特征重要性的判断。
代码：fi.py、bike.py
七、简单分类
x1 x2 -> y
3   1        0
2   5        1
1   8        1
6   4        0
5   2        0
3   5        1
4   7        1
4  -1        0
模型：
if x1 < x2 then y = 1
if x1 > x2 then y = 0
2   9        ? -> 1
7   3        ? -> 0
代码：simple.py







